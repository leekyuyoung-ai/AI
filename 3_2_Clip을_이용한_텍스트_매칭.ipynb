{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ENSyc6HNhcl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP(Constrative Language-Image Pre-training) OpenAI에서만든 모델\n",
        "  - 이미지와 텍스트를 동일한 벡터공간으로 매핑해서 이미지-텍스트 매칭 및 다양한 멀티모달 작업을 수행\n",
        "  - 이미지와 텍스트를 쌍으로하는 대규모 데이터셋을 사용해서 훈련, 이를 통해 이미지와 텍스트 간의 의미적 관계를 학습  "
      ],
      "metadata": {
        "id": "KGAVxEWNNmUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 작동방식\n",
        "  - 입력처리: 이미지는 CNN 또는 Vit 기반 이미지 인코더에 의해서 벡터로변환\n",
        "  - 텍스트 : BERT 와 유사한 Transformer 기반 텍스트 인코더를 통해 벡터로 변환\n",
        "- 멀티모달 표현:\n",
        "  - 이미지와 텍스트는 각각 512차원의 벡터로 변환\n",
        "- 학습\n",
        "  - 정답 이미지-텍스트 쌍의 점수를 최대화하고 , 오답쌍의점수를 최소화 하도록 훈련    "
      ],
      "metadata": {
        "id": "GW6WPbKwOcfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 응용\n",
        "  - 이미지 검색\n",
        "  - 텍스트 생성\n",
        "  - 이미지 분류"
      ],
      "metadata": {
        "id": "Y1JMOFthPAwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dgascAT-Obn2",
        "outputId": "1af8bf93-117f-4f94-ddcc-9ea0230e7bb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --yes\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy\n",
            "Successfully installed ftfy-6.3.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-n18az__n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-n18az__n\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=822e65d31da18be366dfa5c7f811b9ccde6ae0ddf12cc6f702d91ccf4702bbb0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i9vpt4tu/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "# 모델 토크나이져 불러오기"
      ],
      "metadata": {
        "id": "QQIhGfljP2S_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npZJrRPzQdNQ",
        "outputId": "3370f83b-1085-4825-95bd-f7ac7533ee7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 114MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_lists = [\"a dog\", \"a cat\", \"a bird\",'a human']"
      ],
      "metadata": {
        "id": "_t2X_-XoQtRG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지와 텍스트 준비\n",
        "image = preprocess(Image.open(\"cute_dog.jpg\")).unsqueeze(0).to(device)\n",
        "texts = clip.tokenize(text_lists).to(device)"
      ],
      "metadata": {
        "id": "82bStsV4RFwH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지와 텍스트 임베딩 계산\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(texts)"
      ],
      "metadata": {
        "id": "4r-9ny51R7zf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 계산(코사인 유사도)\n",
        "# 노름 norm  : 벡터정규화 또는 노름연산을 수행하는 데 사용  벡터의 길이를 계산(유클리드 거리)\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)"
      ],
      "metadata": {
        "id": "aOskXVZMSDl2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity.cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpAjK9v6Sn_f",
        "outputId": "536f9d19-59b9-451b-d898-c529a9909288"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9789838 , 0.00871402, 0.0029617 , 0.00934043]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR-100 데이터셋 활용"
      ],
      "metadata": {
        "id": "miMuxwq7Vw3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH1ifj9WSpOG",
        "outputId": "114ba035-5b12-4276-9a60-4bc917a721f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:03<00:00, 101MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /root/.cache/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:12<00:00, 13.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /root/.cache/cifar-100-python.tar.gz to /root/.cache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "image, label = cifar100[100]\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image)\n",
        "plt.title(cifar100.classes[label])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "mqJQZYYFWX7p",
        "outputId": "9f7400cf-8fcc-4ecd-eae1-2548d830a399"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100.classes"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZE3yPU8Yb9z",
        "outputId": "3f893231-e821-4b9b-b7b9-8a6c8bbf0dd0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple',\n",
              " 'aquarium_fish',\n",
              " 'baby',\n",
              " 'bear',\n",
              " 'beaver',\n",
              " 'bed',\n",
              " 'bee',\n",
              " 'beetle',\n",
              " 'bicycle',\n",
              " 'bottle',\n",
              " 'bowl',\n",
              " 'boy',\n",
              " 'bridge',\n",
              " 'bus',\n",
              " 'butterfly',\n",
              " 'camel',\n",
              " 'can',\n",
              " 'castle',\n",
              " 'caterpillar',\n",
              " 'cattle',\n",
              " 'chair',\n",
              " 'chimpanzee',\n",
              " 'clock',\n",
              " 'cloud',\n",
              " 'cockroach',\n",
              " 'couch',\n",
              " 'crab',\n",
              " 'crocodile',\n",
              " 'cup',\n",
              " 'dinosaur',\n",
              " 'dolphin',\n",
              " 'elephant',\n",
              " 'flatfish',\n",
              " 'forest',\n",
              " 'fox',\n",
              " 'girl',\n",
              " 'hamster',\n",
              " 'house',\n",
              " 'kangaroo',\n",
              " 'keyboard',\n",
              " 'lamp',\n",
              " 'lawn_mower',\n",
              " 'leopard',\n",
              " 'lion',\n",
              " 'lizard',\n",
              " 'lobster',\n",
              " 'man',\n",
              " 'maple_tree',\n",
              " 'motorcycle',\n",
              " 'mountain',\n",
              " 'mouse',\n",
              " 'mushroom',\n",
              " 'oak_tree',\n",
              " 'orange',\n",
              " 'orchid',\n",
              " 'otter',\n",
              " 'palm_tree',\n",
              " 'pear',\n",
              " 'pickup_truck',\n",
              " 'pine_tree',\n",
              " 'plain',\n",
              " 'plate',\n",
              " 'poppy',\n",
              " 'porcupine',\n",
              " 'possum',\n",
              " 'rabbit',\n",
              " 'raccoon',\n",
              " 'ray',\n",
              " 'road',\n",
              " 'rocket',\n",
              " 'rose',\n",
              " 'sea',\n",
              " 'seal',\n",
              " 'shark',\n",
              " 'shrew',\n",
              " 'skunk',\n",
              " 'skyscraper',\n",
              " 'snail',\n",
              " 'snake',\n",
              " 'spider',\n",
              " 'squirrel',\n",
              " 'streetcar',\n",
              " 'sunflower',\n",
              " 'sweet_pepper',\n",
              " 'table',\n",
              " 'tank',\n",
              " 'telephone',\n",
              " 'television',\n",
              " 'tiger',\n",
              " 'tractor',\n",
              " 'train',\n",
              " 'trout',\n",
              " 'tulip',\n",
              " 'turtle',\n",
              " 'wardrobe',\n",
              " 'whale',\n",
              " 'willow_tree',\n",
              " 'wolf',\n",
              " 'woman',\n",
              " 'worm']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the inputs\n",
        "image, class_id = cifar100[100]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)  # 4차원형태\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fahfHMcXW-ZM",
        "outputId": "2dbb9af7-1aef-40ad-dd3a-092f222e3bb2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top predictions:\n",
            "\n",
            "            crab: 21.95%\n",
            "          spider: 19.67%\n",
            "   aquarium_fish: 8.59%\n",
            "       pine_tree: 5.29%\n",
            "       palm_tree: 4.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vit-B/32 외 다른 모델을 사용"
      ],
      "metadata": {
        "id": "KOCEJ6j-aWgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip.available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHvsIugmW-bx",
        "outputId": "ff4c9c65-b58e-4e42-f83f-4b10f68969c7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CNN 대신에 이미지의 특성을 추출하는 역활을 CLIP이대체하고\n",
        "- 그 데이터를 받아서 기계학습에 적용한다"
      ],
      "metadata": {
        "id": "QZyxRMWXcCIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Load the dataset\n",
        "root = os.path.expanduser(\"~/.cache\")\n",
        "train = CIFAR100(root, download=True, train=True, transform=preprocess)\n",
        "test = CIFAR100(root, download=True, train=False, transform=preprocess)\n",
        "\n",
        "# 특징 추출 함수 - CNN 대처\n",
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n",
        "\n",
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(train)\n",
        "test_features, test_labels = get_features(test)\n",
        "\n",
        "# Perform logistic regression\n",
        "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\n",
        "classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "predictions = classifier.predict(test_features)\n",
        "accuracy = np.mean((test_labels == predictions).astype(float)) * 100.\n",
        "print(f\"Accuracy = {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBd6RLlKW-eR",
        "outputId": "c0091cb2-b099-43f4-a442-7ff59c721c61"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:54<00:00,  4.36it/s]\n",
            "100%|██████████| 100/100 [00:22<00:00,  4.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 72.430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  사용자가 입력한 텍스트 설명(ex: black color bag) 와 제품 이미지를 매칭해서 검색 결과를 제공\n",
        " - few shot learning 을 사용"
      ],
      "metadata": {
        "id": "IDAalFAae7AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터\n",
        "image_paths = [\n",
        "    'bag1.jpg','bag2.jpg','bag3.jpg','bag4.jpg','bag5.jpg',\n",
        "]\n",
        "texts = [\n",
        "    'Large Canvas Tote Bag for Women,Womens Laptop Work Book Bags Crossbody Purse Handbag Shoulder Travel Messenger Gym Totes',\n",
        "    'PAZIMIIK Corduroy Tote Bags for Women Travel Reusable Grocery Shopping Shoulder Go-to Everyday Bag for Work',\n",
        "    'Tactical Camping Storage Bag Utility Tote Bag Camping Kitchen Organizer with Axe Holder & Shoulder Strap',\n",
        "    'Laptop Tote Bag for Women 15.6 Inch Waterproof Leather Computer Bags Women Business Office Work Bag Briefcase Brown',\n",
        "    'Valleycomfy Tote Bag for Women Corduroy Crossbody Tote Bag Purse Women Travel Shoulder Bags Work Handbags Everyday Hobo Bag'\n",
        "]\n",
        "# 이미지 전처리 및 텍스트 토크나이져\n",
        "images = [ preprocess(Image.open(image_path)).unsqueeze(0).to(device) for image_path in image_paths]\n",
        "text_inputs = clip.tokenize(texts).to(device)\n",
        "\n",
        "# 특징 추출\n",
        "with torch.no_grad():\n",
        "    image_features = torch.cat([model.encode_image(image) for image in images])\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# 특징 정규화\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# 유사도 계산\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "# 유사도 기반 가장 매칭이 잘된 이미지를 선택\n",
        "for i, img_path in enumerate(image_paths):\n",
        "  top_match = similarity[i].argmax().item()\n",
        "  print(f'image {img_path} match text : {texts[top_match]}')\n",
        "\n",
        "# 학습결과를 바탕으로 텍스트 기반 이미지 매칭\n",
        "search_query = input(\"찾고자하는 가방에대해 설명해 주세요(영어로)\")\n",
        "text_inputs = clip.tokenize([search_query]).to(device)\n",
        "\n",
        "# 특징 추출\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "# 유사도 계산\n",
        "simularity = image_features @ text_features.T\n",
        "most_simularity_index = simularity.argmax().item()\n",
        "print(f'most simularity of images : {image_paths[most_simularity_index]}')"
      ],
      "metadata": {
        "id": "LPtK6AhQW-g7"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-S5Q2lnAr8HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwfWNYCNqMwg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}