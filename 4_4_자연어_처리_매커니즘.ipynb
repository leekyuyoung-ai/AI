{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85LiALYRaC-t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 어텐션 매커니즘(Attention Mechanism)\n",
        "  - 자연어 처리와 컴퓨터비전에 서 핵심역활\n",
        "  - 시퀀스 데이터 처리에서 특정 부분에 집중할 수 있도록 도와준다  \n",
        "- 기존 RNN 문제점\n",
        "  -  RNN ,LSTM 순차 데이터를 다룰수 있고 긴 문장을 처리할때 다음과 같은 한계\n",
        "    - 장기 의존성 문제\n",
        "      - 입력 길이가 길어질수록 초반 입력의 정보가 후반에 잘 전달되지 않음(그레이디언트 소실)\n",
        "      - 병렬 연산불가\n",
        "        - 시퀀스데이터 특성상 병렬 처리가 어렵고 학습 속도가 느림\n",
        "      - 고정된 컨텍스트 벡터 문제\n",
        "        - 기존 seq2seq 모델에서는 전체 문장을 하나의  벡터로 인코딩하므로, 정보손실이 발생"
      ],
      "metadata": {
        "id": "s_0IG0fraqnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 구조\n",
        "  - 인코더와 디코더로 구성된 모델\n",
        "  - 각각 인코더와 디코더 블럭은 Multi-Head Self-Attention + Feed Forward Network(FFN)\n",
        "  - Self Attention\n",
        "    - 트랜스포커는 각 단어가 문장 내 다른 단어와 얼마나 관련 있는지 계산하는 것\n",
        "    - 기존 RNN과 다른점은 모든 단어를 한 번에 고려할 수 있도록 한다\n",
        "    - 입력을 Query(Q), key(k), value(V)로 변환\n",
        "    - 어텐션 스코어 계산\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "GCQp0arcb_y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h84VVKomdhuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "소프트맥스 정규화 후 가중치 적용\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "kn9wQ64vct4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션 코드"
      ],
      "metadata": {
        "id": "5KC8VZ6Ydi8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "OgipgYQlbqb6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 정의 모델(토치의 모델클래스를 상속받아서)\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "  # 초기화\n",
        "  def __init__(self, d_k):\n",
        "    super(ScaledDotProductAttention, self).__init__()  # 부모의 속성을 상속받아서 사용 할 수 있도록\n",
        "    self.d_k = d_k\n",
        "  # 어텐션 연산  함수이름이 forward로 정해져있음.. 즉.. 부모의 forward를 재 정의\n",
        "  def forward(self, Q, K ,V):\n",
        "    # K.transpose(-2, -1) # Key 행렬의 차원을 변경하여 행렬 곱 연산 가능하게 조정\n",
        "    # torch.matmul(Q, K.transpose(-2, -1))  Q 와 K를 내적해서 유사도 점수 Attention Score 계산\n",
        "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k,dtype=torch.float32))  # Sfotmax적용전 스케일링 연산\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)  # softmax함수를 적용해 확률 값(가중치)을 계산\n",
        "    output = torch.matmul(attention_weights, V) # 어텐션 가중치와 value 행렬을 곱해서 최종 출력을 생성\n",
        "    return output, attention_weights,attention_scores\n"
      ],
      "metadata": {
        "id": "LVcmRXpYd2EC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테스트 데이터 생성 및 실행"
      ],
      "metadata": {
        "id": "LSxz8fqBfzbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = 64 # key의 차원\n",
        "batch_size, seq_len, d_model = 2,5,64\n",
        "# 배치크기 :2 한번에 2개의 문장을 처리\n",
        "# 시퀀스길이 : 5 각 문장이 5개의 단어로 구성\n",
        "# 모델 차원 : 64  각 단어를 64차원 벡터로 표현\n",
        "\n",
        "Q = torch.rand(batch_size, seq_len, d_model)\n",
        "K = torch.rand(batch_size, seq_len, d_model)\n",
        "V = torch.rand(batch_size, seq_len, d_model)"
      ],
      "metadata": {
        "id": "PhpebLZSfvcy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention = ScaledDotProductAttention(d_k)\n",
        "output, attention_weights,attention_scores = attention(Q, K, V)\n"
      ],
      "metadata": {
        "id": "kxoVNZSqglIL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"어텐션 출력 크기:\",output.shape)\n",
        "print(\"어텐션 가중치 크기:\",attention_weights.shape)\n",
        "print(\"어텐션 스코어 크기:\",attention_scores.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKCXNbVTgx4R",
        "outputId": "b27fb531-bc45-44f2-b114-23dd1f36a352"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어텐션 출력 크기: torch.Size([2, 5, 64])\n",
            "어텐션 가중치 크기: torch.Size([2, 5, 5])\n",
            "어텐션 스코어 크기: torch.Size([2, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAVqS7fKhe0y",
        "outputId": "43ba3cfe-b6ff-423b-e4bb-80a8da5deed9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2.2319, 1.9291, 2.0628, 1.7988, 2.5103],\n",
              "         [2.2370, 2.0451, 2.2670, 1.9474, 2.6199],\n",
              "         [2.2245, 2.1048, 2.1830, 1.8557, 2.6288],\n",
              "         [1.9262, 1.8138, 1.9285, 1.6872, 2.2015],\n",
              "         [1.9930, 1.8105, 2.0162, 1.7061, 2.2203]],\n",
              "\n",
              "        [[1.8207, 2.2764, 2.2091, 2.3287, 2.1513],\n",
              "         [1.9785, 2.3894, 2.4953, 2.5732, 2.2327],\n",
              "         [1.9537, 2.2079, 2.2572, 2.3654, 2.1794],\n",
              "         [1.7433, 1.8932, 2.0319, 2.1063, 1.9805],\n",
              "         [2.0100, 2.1099, 2.2333, 2.3094, 2.0905]]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_scores[0,0,2]  # 첫번째 문장의 첫번째 단어와 세번째 단어의 유사도 점수가 2.0628"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm35Nj3yhuoZ",
        "outputId": "12ee27a0-2aeb-45ca-a8f8-385acd0e0754"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0628)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights  # 각 행의 합이 1이 되는 확률 값이다  한 단어가 다른 단어에 얼마나 영향을 주는지 표현"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W34rlWZhfoJ",
        "outputId": "523986ef-0499-4a18-8873-84dbb893b98d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2196, 0.1623, 0.1855, 0.1424, 0.2902],\n",
              "         [0.1972, 0.1628, 0.2032, 0.1476, 0.2892],\n",
              "         [0.1986, 0.1762, 0.1905, 0.1373, 0.2975],\n",
              "         [0.2000, 0.1787, 0.2004, 0.1575, 0.2634],\n",
              "         [0.2057, 0.1713, 0.2105, 0.1544, 0.2581]],\n",
              "\n",
              "        [[0.1407, 0.2220, 0.2075, 0.2339, 0.1959],\n",
              "         [0.1372, 0.2070, 0.2301, 0.2487, 0.1770],\n",
              "         [0.1561, 0.2013, 0.2114, 0.2356, 0.1956],\n",
              "         [0.1612, 0.1873, 0.2152, 0.2318, 0.2044],\n",
              "         [0.1728, 0.1909, 0.2160, 0.2331, 0.1872]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights[0,0,2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ASxm7VbiWoJ",
        "outputId": "ef882fb5-ac8a-4662-e083-add4386afee9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1855)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum([0.2196, 0.1623, 0.1855, 0.1424, 0.2902])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po68iFXhhiHZ",
        "outputId": "94c87edf-8a1f-45a5-ce4f-4718ef2a5e29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머 모델 주요 구성 요소\n",
        " - Positional Encoding: 단어의 순서 정보를 추가하는 역할\n",
        " - Multi-Head Attention: 여러 개의 Self-Attention을 병렬로 적용\n",
        " - Feed Forward Network: 비선형 변환을 수행하는 완전연결층\n",
        " - Layer Normalization & Residual Connection: 학습을 안정화시키는 역할\n",
        " - Transformer Encoder Block: 위 요소들을 조합하여 Transformer를 구성"
      ],
      "metadata": {
        "id": "ohN5gZsAlvOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding\n",
        "  - Transformer 모델은 CNN이나 RNN처럼 단어 순서를 고려하지 않음.\n",
        "  - 하지만 자연어 처리에서는 단어의 순서 정보가 중요하므로,\n",
        "  - Positional Encoding을 사용하여 위치 정보를 추가한다.\n",
        "  - 우리는 sin, cos 함수를 이용한 고유한 위치 정보를 생성"
      ],
      "metadata": {
        "id": "2CTbUOUsl-rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model) # (max_len, d_model) 크기의 행렬 생성\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # 위치값 (0 ~ max_len)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # 짝수 인덱스에 sin 함수 적용\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # 홀수 인덱스에 cos 함수\n",
        "        pe = pe.unsqueeze(0) # 배치 차원 추가\n",
        "        self.register_buffer('pe', pe) # 학습되지 않은 값으로 저장\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "uY1sm7V-miOB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Attention\n",
        "  - 각각의 어텐션 헤드가 서로 다른 정보를 학습할 수 있다"
      ],
      "metadata": {
        "id": "hJdVQgLBn9zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0  # num_heads는 d_model의 약수여야 함\n",
        "        self.d_k = d_model // num_heads  # 각 헤드의 차원\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # W_q, W_k, W_v: Query, Key, Value를 위한 가중치 행렬\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # 최종 출력 변환\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        # Query, Key, Value를 (batch, seq_len, d_model) → (batch, seq_len, num_heads, d_k)로 변환\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention 적용\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "        attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # (batch, num_heads, seq_len, d_k) → (batch, seq_len, d_model)로 변환   마지막에 헤드를 다시 합쳐서 출력 차원을 유지\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "        return self.W_o(output)\n"
      ],
      "metadata": {
        "id": "wf0tK4b7oOhZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward network (FFN)\n",
        "  - 완전연결층을 적용해서 비선형 변환을 수행"
      ],
      "metadata": {
        "id": "6f7iGmvRolXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "     super(FeedForward, self).__init__()\n",
        "     self.linear1 = nn.Linear(d_model, d_ff)\n",
        "     self.relu = nn.ReLU()\n",
        "     self.linear2 = nn.Linear(d_ff, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.relu(self.linear1(x)))"
      ],
      "metadata": {
        "id": "owZKRGnSouUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder Layer (단일 레이어)\n",
        "  - Multi_Head Attenrion + FFN + LayerNorm"
      ],
      "metadata": {
        "id": "igJ7bu4gpQfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "yRC2DRRqpZjx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Encoder (전체 모델)\n",
        "  - 여러개의 Encoder Layer를 쌓아서 모델을 구성"
      ],
      "metadata": {
        "id": "Q5BNn7-zpfaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_len):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n"
      ],
      "metadata": {
        "id": "jsTrs5wbpaNR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8SsAO0XyplXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}