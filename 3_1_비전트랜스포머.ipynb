{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEzgv1uqbkiB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "비전트랜스포머(Vit)\n",
        "  - 컴퓨터 비전문제를 처리하기위해서 트랜스포머 아키텍처를 사용하는 모델\n",
        "  - Vit는 이미지를 고정크기의 패치로 나누고 각 패치를 선형변환을 통해서 임베딩 벡터로 변환해서 입력으로 사용해서 self attention 매커니즘을 통해 이미지의 특징을 학습"
      ],
      "metadata": {
        "id": "G4ruhvYrbrGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT의 주요 구성 요소\n",
        "  1. 이미지패치를 분할\n",
        "    - 이미지를 N x N 크기의 작은 패치로 나눕니다\n",
        "    - ex) 224 x 224 이미지를 16 x 16 패치로나누면 14 x 14 * 196개의 패치가생성\n",
        "  2. 패치 임베딩\n",
        "    - 각 패치는 16 x 16 x 3 크기의 벡터로 표현, 이를 선형 변환하여 고정된 차원의 임베딩으로 변환\n",
        "    - ex) 16 x 16 x3 = 768 임베딩 차원 D = 512\n",
        "  3. 위치 임베딩\n",
        "     - 트랜스포머는 순서정보가 없다, 패치의 순서를 나타내기 위해 위치 임베딩을 추가\n",
        "  4. 트랜스포머 인코더\n",
        "    - 트랜스포머 아키텍처를 사용하여 패치 임베딩 간의 관계를 학습\n",
        "      - 멀티헤드 셀프 어텐션\n",
        "      - 피드포워드 네트웍\n",
        "  5. 분류 토근(Class Token)\n",
        "    - 추가적인 [CLS] 토큰을 삽입하여 최종적으로 이 토큰을 통해 이미지 분류 결과를 출력          \n",
        "\n"
      ],
      "metadata": {
        "id": "wb4gaTu4cSvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import Compose,Resize,ToTensor"
      ],
      "metadata": {
        "id": "D5X3yiFTemNu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * 3\n",
        "\n",
        "        # Patch + Position Embeddings\n",
        "        self.patch_to_embedding = nn.Linear(self.patch_dim, dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        # Classifier head\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch_size = img.shape[0]\n",
        "        patches = img.unfold(2, 16, 16).unfold(3, 16, 16)\n",
        "        patches = patches.contiguous().view(batch_size, -1, self.patch_dim)\n",
        "        tokens = self.patch_to_embedding(patches)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, tokens), dim=1)\n",
        "        x += self.positional_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)  # Use TransformerEncoder\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "# Example usage\n",
        "transform = Compose([Resize((224, 224)), ToTensor()])\n",
        "vit_model = VisionTransformer()\n",
        "dummy_image = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB Image\n",
        "output = vit_model(dummy_image)\n",
        "print(output.shape)  # Output logits for each class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-eZakvncQRv",
        "outputId": "f42133c8-c8b2-4b7f-ee4d-2779defffcdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "주요 단계\n",
        "1. 데이터 준비\n",
        "  - CIFAR-10 데이터를 로드하고 전처리합니다.\n",
        "  - 데이터 증강을 사용하여 성능을 향상시킵니다.\n",
        "2. 모델 초기화\n",
        "  - Vision Transformer를 생성합니다.\n",
        "3. 손실 함수 및 옵티마이저 정의\n",
        "  - CrossEntropyLoss와 AdamW 옵티마이저를 사용합니다.\n",
        "4. 학습 루프\n",
        "  - 배치별로 데이터를 처리하고 손실을 계산하여 역전파를 수행합니다.\n",
        "  - 학습 진행 상황을 출력합니다."
      ],
      "metadata": {
        "id": "vriSCc6tnWO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ViT는 고정된 입력 크기를 요구합니다.\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# CIFAR-10 데이터셋 로드\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Vision Transformer 모델 정의 (이전 코드 사용)\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, num_classes=10, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * 3\n",
        "\n",
        "        self.patch_to_embedding = nn.Linear(self.patch_dim, dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch_size = img.shape[0]\n",
        "        patches = img.unfold(2, 16, 16).unfold(3, 16, 16)\n",
        "        patches = patches.contiguous().view(batch_size, -1, self.patch_dim)\n",
        "        tokens = self.patch_to_embedding(patches)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, tokens), dim=1)\n",
        "        x += self.positional_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx6AFINTnip5",
        "outputId": "9f4fe54d-c8a5-4bde-e7c8-32cbfc65dc35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = VisionTransformer(num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "HUszmB4wpGca"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "DZ8YST_xpmdb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 루프\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Y8FLs34lqFc5",
        "outputId": "ac4080d5-bbc4-4ac7-bbf6-2fa122c24411"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.3486\n",
            "Epoch [2/10], Loss: 2.3216\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12288529f452>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 루프\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on test set: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "PZhKFgqNrUVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKFmKro19CB7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}