{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEzgv1uqbkiB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "비전트랜스포머(Vit)\n",
        "  - 컴퓨터 비전문제를 처리하기위해서 트랜스포머 아키텍처를 사용하는 모델\n",
        "  - Vit는 이미지를 고정크기의 패치로 나누고 각 패치를 선형변환을 통해서 임베딩 벡터로 변환해서 입력으로 사용해서 self attention 매커니즘을 통해 이미지의 특징을 학습"
      ],
      "metadata": {
        "id": "G4ruhvYrbrGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViT의 주요 구성 요소\n",
        "  1. 이미지패치를 분할\n",
        "    - 이미지를 N x N 크기의 작은 패치로 나눕니다\n",
        "    - ex) 224 x 224 이미지를 16 x 16 패치로나누면 14 x 14 * 196개의 패치가생성\n",
        "  2. 패치 임베딩\n",
        "    - 각 패치는 16 x 16 x 3 크기의 벡터로 표현, 이를 선형 변환하여 고정된 차원의 임베딩으로 변환\n",
        "    - ex) 16 x 16 x3 = 768 임베딩 차원 D = 512\n",
        "  3. 위치 임베딩\n",
        "     - 트랜스포머는 순서정보가 없다, 패치의 순서를 나타내기 위해 위치 임베딩을 추가\n",
        "  4. 트랜스포머 인코더\n",
        "    - 트랜스포머 아키텍처를 사용하여 패치 임베딩 간의 관계를 학습\n",
        "      - 멀티헤드 셀프 어텐션\n",
        "      - 피드포워드 네트웍\n",
        "  5. 분류 토근(Class Token)\n",
        "    - 추가적인 [CLS] 토큰을 삽입하여 최종적으로 이 토큰을 통해 이미지 분류 결과를 출력          \n",
        "\n"
      ],
      "metadata": {
        "id": "wb4gaTu4cSvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import Compose,Resize,ToTensor"
      ],
      "metadata": {
        "id": "D5X3yiFTemNu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=224, patch_size=16, num_classes=1000, dim=768, depth=12, heads=12, mlp_dim=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_dim = patch_size * patch_size * 3\n",
        "\n",
        "        # Patch + Position Embeddings\n",
        "        self.patch_to_embedding = nn.Linear(self.patch_dim, dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=mlp_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        # Classifier head\n",
        "        self.to_cls_token = nn.Identity()\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        batch_size = img.shape[0]\n",
        "        patches = img.unfold(2, 16, 16).unfold(3, 16, 16)\n",
        "        patches = patches.contiguous().view(batch_size, -1, self.patch_dim)\n",
        "        tokens = self.patch_to_embedding(patches)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, tokens), dim=1)\n",
        "        x += self.positional_embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer_encoder(x)  # Use TransformerEncoder\n",
        "        x = self.to_cls_token(x[:, 0])\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "# Example usage\n",
        "transform = Compose([Resize((224, 224)), ToTensor()])\n",
        "vit_model = VisionTransformer()\n",
        "dummy_image = torch.randn(1, 3, 224, 224)  # Batch size 1, RGB Image\n",
        "output = vit_model(dummy_image)\n",
        "print(output.shape)  # Output logits for each class\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-eZakvncQRv",
        "outputId": "3f54a23f-309c-4056-8c7c-b07efdf88786"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    }
  ]
}