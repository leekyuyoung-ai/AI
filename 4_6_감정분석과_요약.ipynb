{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "토큰화(Tokenization)는 자연어 처리에서 코퍼스를 의미 있는 단위로 나누는 과정입니다. 주요 개념과 고려할 사항은 다음과 같습니다.\n",
        "\n",
        "1. 단어 토큰화 (Word Tokenization)\n",
        "  - 단어를 기준으로 텍스트를 나누는 작업입니다.\n",
        "  - 구두점(punctuation) 제거가 일반적이지만 의미를 잃을 수도 있습니다.\n",
        "  - 영어에서는 아포스트로피 등의 처리 방식이 다를 수 있으며, NLTK, Keras 등을 활용한 다양한 방법이 존재합니다.\n",
        "\n",
        "2. 토큰화 중 선택의 문제\n",
        "  - 예를 들어 \"Don't\"는 Do n't, Dont, Don ' t 등으로 분리할 수 있습니다.\n",
        "  - NLTK의 word_tokenize, WordPunctTokenizer, Keras의 text_to_word_sequence 등의 도구들이 다른 방식으로 토큰화를 수행합니다.\n",
        "\n",
        "3. 토큰화 시 고려 사항\n",
        "  - 구두점이나 특수문자를 무조건 제거하면 의미가 사라질 수 있음.\n",
        "  - 단어 내부에 띄어쓰기가 포함된 경우 고려 필요 (예: \"New York\").\n",
        "  - 표준 토큰화 방식 (예: Penn Treebank Tokenization)에서는 하이픈 단어를 유지하고, 아포스트로피를 분리하는 규칙 적용.\n",
        "\n",
        "4. 문장 토큰화 (Sentence Tokenization)\n",
        "  - 문장을 기준으로 텍스트를 나누는 작업.\n",
        "  - NLTK의 sent_tokenize는 마침표(.)가 포함된 약어(ex. Ph.D.) 등을 적절히 처리함.\n",
        "   -한국어에서는 KSS(Korean Sentence Splitter) 같은 도구를 활용.\n",
        "  \n",
        "5. 한국어에서의 토큰화의 어려움\n",
        "  - 한국어는 교착어 특성상 단순 띄어쓰기 기반 토큰화가 어렵고, 형태소 분석이 필요.\n",
        "  - 형태소 분석기 (KoNLPy의 Okt, Komoran, Hannanum, Mecab) 사용이 일반적."
      ],
      "metadata": {
        "id": "UAT6VQDeZPU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK, Keras, KoNLPy, KSS 단어 및 문장 토큰화"
      ],
      "metadata": {
        "id": "XhU87Q_Ubex_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk kss konlpy tensorflow"
      ],
      "metadata": {
        "id": "PxMmRDBQZpd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa2dfcfa-0968-4bd9-fce3-8c1be1f6b3d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting kss\n",
            "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pecab (from kss)\n",
            "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from kss) (3.4.2)\n",
            "Collecting jamo (from kss)\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting hangul-jamo (from kss)\n",
            "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
            "Collecting tossi (from kss)\n",
            "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting distance (from kss)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyyaml==6.0 (from kss)\n",
            "  Downloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting unidecode (from kss)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting cmudict (from kss)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting koparadigm (from kss)\n",
            "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting kollocate (from kss)\n",
            "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting bs4 (from kss)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kss) (1.26.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from kss) (8.3.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from kss) (1.13.1)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->kss) (4.13.3)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->kss) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->kss) (6.5.2)\n",
            "Collecting whoosh (from kollocate->kss)\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from pecab->kss) (17.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->kss) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->kss) (1.5.0)\n",
            "Collecting bidict (from tossi->kss)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->kss) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->kss) (2.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.9/757.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: kss, distance, pecab, tossi\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-6.0.4-cp311-cp311-linux_x86_64.whl size=1452484 sha256=b35d0f28cf82573078fb143ce5f26a5335e6cc4d974d503fbb79f92cabd961ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/c9/5f/69b8fe9751eefbb5d087932ddf58d0f121b8e545335af7fe4e\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=e332203006aba43a6e1c9576ae108c1b2d764251914838f3e9d222f3b178d5bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/cd/9c/3ab5d666e3bcacc58900b10959edd3816cc9557c7337986322\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646665 sha256=d29bb9b001d45984c1ee47931116609dffa8ec45840ce1a6cf5c2243498952b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/0d/97/ca2bb361e44a80f4c63efe6f6438ff903fd1ab5640eedabc1b\n",
            "  Building wheel for tossi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12130 sha256=ee8e1dd46e23b277f53a4313352e612166f7b73732d77fca7759743a6ef22784\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/1a/7e/0b78039c20678a6682f03cca4295efaa5fb55a3d10d7e9837a\n",
            "Successfully built kss distance pecab tossi\n",
            "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, pyyaml, kollocate, JPype1, bidict, tossi, pecab, koparadigm, konlpy, cmudict, bs4, kss\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 2.0.1\n",
            "    Uninstalling xlrd-2.0.1:\n",
            "      Successfully uninstalled xlrd-2.0.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "Successfully installed JPype1-1.5.2 bidict-0.23.1 bs4-0.0.2 cmudict-1.0.32 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 jamo-0.4.1 kollocate-0.0.2 konlpy-0.6.0 koparadigm-0.10.0 kss-6.0.4 pecab-1.0.8 pyyaml-6.0 tossi-0.3.1 unidecode-1.3.8 whoosh-2.7.4 xlrd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "JNdQcs6EZXw3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcSiL3w9dY8l",
        "outputId": "3a8359cf-fdc8-4c43-fdfa-a8eb2f908d1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어토큰화 (영어)\n",
        "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "metadata": {
        "id": "2hpDvLyNdfUt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word_tokenize:\",word_tokenize(text)) # 공백을 기준으로단어를 분리, 어포스트로피를 적절히 분리\n",
        "print(\"WordPunctTokenizer:\",WordPunctTokenizer().tokenize(text)) # 모든 구두점을 별도로 분리\n",
        "print(\"sent_tokenize:\",text_to_word_sequence(text)) # 소문자로 변환, 구두점을 제거하지만 어포스트로피는 유지"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36X7NNqEdwRF",
        "outputId": "51ac8fc1-1ff4-4296-f56b-62919954903e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize: ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "WordPunctTokenizer: ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n",
            "sent_tokenize: [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화"
      ],
      "metadata": {
        "id": "Zr4R1vHQfAp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff.\""
      ],
      "metadata": {
        "id": "EiSFq4vKeKi1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sent_tokenize(text):  # 마침표를  기준으로 문장을 나누지만 문장 내부의 마침표는 잘 처리\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj-YTuG7fm5G",
        "outputId": "ffd16f52-9798-4406-8808-d9c8ad34ecaa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "His barber kept his word.\n",
            "But keeping such a huge secret to himself was driving him crazy.\n",
            "Finally, the barber went up a mountain and almost to the edge of a cliff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'I am actively looking for Ph.D. students. And you are a Ph.D student.'\n",
        "for sentence in sent_tokenize(text):  # 마침표를  기준으로 문장을 나누지만 문장 내부의 마침표는 잘 처리\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIGWDfZLgAb-",
        "outputId": "4fb4b16c-71f3-4cfd-bf82-fba1f2614bf9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am actively looking for Ph.D. students.\n",
            "And you are a Ph.D student.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어"
      ],
      "metadata": {
        "id": "GnqD7Kpjghft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "text = '자연어 처리는 어렵지만 재미있습니다.'\n",
        "print(\"형태소 단위 토큰화\", okt.morphs(text))\n",
        "print(\"명사 추출\",okt.nouns(text))\n",
        "print(\"품사 태깅\",okt.pos(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9rdFugrgCs9",
        "outputId": "c1e2f5a2-b85f-4a74-c965-1ffadb229952"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "형태소 단위 토큰화 ['자연어', '처리', '는', '어렵지만', '재미있습니다', '.']\n",
            "명사 추출 ['자연어', '처리']\n",
            "품사 태깅 [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('어렵지만', 'Adjective'), ('재미있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - 정제\n",
        "    - 텍스트에서 노이즈 데이터를 제거\n",
        "    - 노이즈 데이터 : 의미없는 특수문자, 중복된 공백, 불필요한 단어등\n",
        "    - 불용어(stopwords)제거, 등장 빈도가 적은 단어 삭제 등의 작업\n",
        "  - 정규화\n",
        "    - 의미는 같지만 다른형태로 표현된 단어를 통합하는 과정\n",
        "    - 방법\n",
        "      - 대소문자 통합: \"KOREA\", \"Korea\", \"korea\"\n",
        "      - 표현 방식 통합 : \"KOREA\", \"KOR\", \"USA\" ,\"US\"\n",
        "      - 어간 추출(Stemming)과 표제어 추출(Lemmatation) : \"running\" -> \"run\" 단어의 기본형으로 변환\n",
        "  - 불필요한 단어 제거\n",
        "    - 등장빈도가 낮은단어\n",
        "    - 짧은 단어(1~2자) 제거\n",
        "  - 정규표현식(Regrex)활용\n",
        "    - 특정패턴제거(html, 날짜, 특수기호)\n",
        "    - 텍스트 정제 과정에서 반복적으로 등장하는 패턴을 처리\n"
      ],
      "metadata": {
        "id": "2jtNt6gEhaiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정제와 정규화 함수"
      ],
      "metadata": {
        "id": "UhZyA2Oxi4gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter  # 등장빈도수구해서 낮은 등장횟수는 제거할 용도\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords # 불용어\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xm05WMThfoE",
        "outputId": "7f661e98-ebad-4009-f4f6-7e4920562aeb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "어간:\n",
        "  - 단어에서 접사(prefix, suffix) 제거\n",
        "  - 의미가 왜곡 될수 있음\n",
        "  - going :  go\n",
        "  - files :  fli  X\n",
        "  - happily  lappili  X\n",
        "  - running run\n",
        "표제어\n",
        "  - 문법적 정보(품사)를 고려해서 사전에 등재된 기본형으로 변환\n",
        "  - 사전을 이용 단어의 원형을 찾을수 있다\n",
        "  - 어간보다 더 복잡한 연산을 수행\n",
        "  - going :  go\n",
        "  - files :  fly\n",
        "  - happily  happy\n",
        "  - running run   \n",
        "\n",
        "어간추출 vs 표제어 추출\n",
        "- 방식  :  단순규칙기반 접사 제거   -  사전 기반 변환\n",
        "- 속도  :   빠름                    - 느림\n",
        "- 정확성 :   낮음(의미변환)         -  높음(문법적 의미 유지)\n",
        "\n",
        "- 빠른 텍스트 분석 : 어간추출\n",
        "- 정화기한 의미를 보전 : 표제어 추출\n",
        "- 대량의 데이터에서 성능 최적화 : 어간 추출\n",
        "- 언어의 정확성이 중요한 모델 : 표제어 추출\n",
        "\n"
      ],
      "metadata": {
        "id": "fBAvIwwMlgNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정제와정규화(대소문자, 특수문자, 불용어&짧은단어,\n",
        "# 어간&표제어 추출)\n",
        "def clean_text(text):\n",
        "  # 대소문자\n",
        "  text = text.lower()\n",
        "  # 특수문자 제거\n",
        "  text = re.sub('[^a-zA-Z\\s]', '', text)\n",
        "  # 불용어&짧은단어\n",
        "  words = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))  # 불용어 사전을 가져온다\n",
        "  words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "  # 표기정규화(어간, 표제어)\n",
        "  stemmer = PorterStemmer()  #어간\n",
        "  lemmatizer = WordNetLemmatizer() # 표제어\n",
        "  stemmed_words = [stemmer.stem(word) for word in words]\n",
        "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "  return ' '.join(stemmed_words), ' '.join(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "YIJUUyDpjUaN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"I was wondering if anyone out there could enlighten me on this car. US and USA are similar. The automobile is expensive!\"\n",
        "# 정제후 결과\n",
        "stemmed_text, lemmatized_text =  clean_text(test_text)"
      ],
      "metadata": {
        "id": "Tp04Ng-HpynU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmed_text)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4nQqqRQp3gj",
        "outputId": "57d85b1f-fe84-416b-a31d-7a2d02eb6b62"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wonder anyon could enlighten car usa similar automobil expens\n",
            "wondering anyone could enlighten car usa similar automobile expensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어(stop_words)\n",
        "  - 자주 등장하지만 의미 분석에 큰 기여를 하지 안히는 단어\n",
        "  - 영어 : i my me the is an  over\n",
        "  - 한국어 : 조사(를, 에서) 접속사(그리고, 그러나), 관형사(이 그 저)"
      ],
      "metadata": {
        "id": "InFQUW0hqtQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "# 불용어 확인\n",
        "stop_words_list = stopwords.words(\"english\")\n",
        "stop_words_list[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8mH1EkyrWJl",
        "outputId": "d879d6e6-2bd9-4f65-9c3e-01e700b68b9d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK를 이용한 불용어제거\n",
        "text = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words = set(stop_words + ['.',\"'s\"])  # 사용자가 추가한 stopword(불용어)\n",
        "# 문장을 토큰화\n",
        "word_tokens =  word_tokenize(text)\n",
        "\n",
        "result = [ word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "print('제거 전',word_tokens)\n",
        "print('제거 후',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBPmhgrHsBf0",
        "outputId": "637a827d-b291-488d-f7a5-e6ad5a29633b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제거 전 ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "제거 후 ['Family', 'important', 'thing', 'It', 'everything']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_text(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K35ZZ_yQqcY0",
        "outputId": "05b63487-7dee-4f94-c19c-c44a5ab5e008"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('famili import thing everyth', 'family important thing everything')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/stopwords-iso/stopwords-ko?ref=deep.chulgil.me"
      ],
      "metadata": {
        "id": "AUGn6ho0u2x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm  install stopwords-ko"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSeWy0gzvRQj",
        "outputId": "55b240a4-12fe-44be-fdf6-90202dd38051"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K\n",
            "added 1 package in 2s\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://registry.npmjs.org/stopwords-ko/-/stopwords-ko-0.2.0.tgz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrpN06wTv6lT",
        "outputId": "c5d78518-65e6-4b03-bf78-02fbe240b974"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-24 11:37:56--  https://registry.npmjs.org/stopwords-ko/-/stopwords-ko-0.2.0.tgz\n",
            "Resolving registry.npmjs.org (registry.npmjs.org)... 104.16.25.34, 104.16.0.35, 104.16.1.35, ...\n",
            "Connecting to registry.npmjs.org (registry.npmjs.org)|104.16.25.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5166 (5.0K) [application/octet-stream]\n",
            "Saving to: ‘stopwords-ko-0.2.0.tgz’\n",
            "\n",
            "\rstopwords-ko-0.2.0.   0%[                    ]       0  --.-KB/s               \rstopwords-ko-0.2.0. 100%[===================>]   5.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-02-24 11:37:56 (49.1 MB/s) - ‘stopwords-ko-0.2.0.tgz’ saved [5166/5166]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zxvf stopwords-ko-0.2.0.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAuS19PkwEtD",
        "outputId": "9832ebaa-26a6-44ee-ddb5-91d3f0931c91"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "package/package.json\n",
            "package/.npmignore\n",
            "package/README.md\n",
            "package/LICENSE\n",
            "package/bower.json\n",
            "package/stopwords-ko.json\n",
            "package/.travis.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/package/stopwords-ko.json') as f:\n",
        "  stop_words = json.load(f)"
      ],
      "metadata": {
        "id": "QiGVr_jswSOc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stop_words), stop_words[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxfjkFpGwkwb",
        "outputId": "4c8736ae-fec7-4bc5-de0a-8f57499b6b98"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(679, ['!', '\"', '$', '%', '&'])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어에서 불용어 제거  okt\n",
        "# 한국어 형태소 분석기\n",
        "okt = Okt()\n",
        "example = '화사, 박나래와 \"전화 차단\" 선언 후 \"큰엄마 같아, 사랑해\"'\n",
        "\n"
      ],
      "metadata": {
        "id": "LmsV6URetlFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}