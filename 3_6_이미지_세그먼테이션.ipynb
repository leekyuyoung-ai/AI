{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- 세그먼트 : 이미지를 여러개의 영역으로 나누는 작업으로 컴퓨터의 비전 및 이미지 처리에 사용\n",
        "  - 이진 세그먼트\n",
        "  - 다중 클래스 세그먼트\n",
        "  - 인스턴스 세그먼트\n",
        "    - 같은 클래스의 객체를 개별적으로 구분\n",
        "  - 파노라믹 세그먼트\n",
        "    - 인스턴스 세그먼트와 픽셀 분류의 조합, 모든 픽셀을 객체로 할당하거나 배경으로 분류"
      ],
      "metadata": {
        "id": "s7PoyvfMh0z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 자료 : https://www.tensorflow.org/tutorials/images/segmentation?hl=ko"
      ],
      "metadata": {
        "id": "MXRQ_5oRjSvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "data_url = \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n",
        "data_dir = tf.keras.utils.get_file(origin=data_url, extract=True)"
      ],
      "metadata": {
        "id": "H3bUbhVwwqbz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "# Extract the .tar.gz file manually\n",
        "if data_dir.endswith('.tar.gz'):\n",
        "    # Open the downloaded .tar.gz file and extract it\n",
        "    with tarfile.open(data_dir, \"r:gz\") as tar_ref:\n",
        "        tar_ref.extractall(os.path.dirname(data_dir))\n",
        "\n",
        "    # Check where the data was extracted\n",
        "    extracted_dir = os.path.join(os.path.dirname(data_dir), 'images')\n",
        "    print(f\"Data extracted to: {extracted_dir}\")\n",
        "else:\n",
        "    print(\"The downloaded file is not in the correct format.\")"
      ],
      "metadata": {
        "id": "f6oGfTfFyniS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(extracted_dir)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "usdKQMNqzwcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n",
        "data_dir = tf.keras.utils.get_file(origin=data_url, extract=True)\n",
        "import tarfile\n",
        "import os\n",
        "# Extract the .tar.gz file manually\n",
        "if data_dir.endswith('.tar.gz'):\n",
        "    # Open the downloaded .tar.gz file and extract it\n",
        "    with tarfile.open(data_dir, \"r:gz\") as tar_ref:\n",
        "        tar_ref.extractall(os.path.dirname(data_dir))\n",
        "\n",
        "    # Check where the data was extracted\n",
        "    extracted_dir_annotations = os.path.join(os.path.dirname(data_dir), 'annotations')\n",
        "    print(f\"Data extracted to: {extracted_dir_annotations}\")\n",
        "else:\n",
        "    print(\"The downloaded file is not in the correct format.\")"
      ],
      "metadata": {
        "id": "PrgKTH2b0BcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IA4kDcW60dgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(extracted_dir_annotations+'/trimaps')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fKTIpW_b0RTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = extracted_dir\n",
        "target_dir = extracted_dir_annotations+'/trimaps'\n",
        "img_size = (160,160)\n",
        "num_classes = 4\n",
        "batch_size = 32\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "BA5B4JFk1PsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(input_img_paths), len(target_img_paths))"
      ],
      "metadata": {
        "id": "5vkuwRnY1i_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[ (input_path, target_path) for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10])]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yhyqqByJ19li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이미지확인"
      ],
      "metadata": {
        "id": "ZNVEj0nd2n7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "display(Image(filename=input_img_paths[0]))\n",
        "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[0]))\n",
        "display(img)"
      ],
      "metadata": {
        "id": "b6U9E5UM2vxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i: i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i: i + self.batch_size]\n",
        "        x = np.zeros((batch_size,) + self.img_size + (3,), dtype='float32')\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((batch_size,) + self.img_size + (1,), dtype='uint8')\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size,\n",
        "                           color_mode='grayscale')\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "24_YanPk8Np3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "\n",
        "    ### [First half of the network: downsampling inputs] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(\n",
        "            filters, 1, strides=2, padding='same')(previous_block_activation)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding='same')(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(\n",
        "        num_classes, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "iCz3bLQ_6f1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# refresh RAM\n",
        "keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "RWZEq4N66mbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model build\n",
        "model = get_model(img_size, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "llGvba7m7ATO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "테이터 학습용과 검증용 분할"
      ],
      "metadata": {
        "id": "k3DD-LNO7QBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# 데이터 셋 만들기\n",
        "train_gen = OxfordPets(  batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ],
      "metadata": {
        "id": "6fYlfxc98XvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습"
      ],
      "metadata": {
        "id": "L1jq0kVI8bA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy')\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint('oxford_segmentation.keras', save_best_only=True)\n",
        "]\n",
        "epochs = 15\n",
        "model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fLh3ZmUZ8yyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "aszb-N6283Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vR2dY2mI-1rV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}